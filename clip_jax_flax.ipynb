{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "toc_visible": true,
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Pip install & Imports"
      ],
      "metadata": {
        "id": "A9C8PZsVv4sZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9dqV58hM2NN"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers\n",
        "!pip install git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "id": "gVgSnr2FNADw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/Lightning-AI/lightning.git"
      ],
      "metadata": {
        "id": "taYkRquANBMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations"
      ],
      "metadata": {
        "id": "Sf9jgWTiNIUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q clu"
      ],
      "metadata": {
        "id": "c1Fv4iE4vy0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "import wandb\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "pykSb_W3NDna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import lightning"
      ],
      "metadata": {
        "id": "IPrB78wjNGMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A"
      ],
      "metadata": {
        "id": "sIhQhu2lNHkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from clu import metrics"
      ],
      "metadata": {
        "id": "E6w_iKxHv_uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image encoder\n",
        "\n",
        "Use FlaxResNetModel from huggingface transformers"
      ],
      "metadata": {
        "id": "aq81mz1_ePYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoImageProcessor, FlaxResNetModel, FlaxViTModel"
      ],
      "metadata": {
        "id": "cBnzAXGsePGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flax import linen as nn"
      ],
      "metadata": {
        "id": "Mvrk2jbPxYW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "  model_name: str\n",
        "\n",
        "  def setup(self):\n",
        "    self.model = FlaxResNetModel.from_pretrained(self.model_name)\n",
        "    #self.model = FlaxViTModel.from_pretrained(self.model_name)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.model(x)"
      ],
      "metadata": {
        "id": "dltZZaDdy2uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Encoder"
      ],
      "metadata": {
        "id": "aAFyZIDbkZNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import FlaxAutoModel"
      ],
      "metadata": {
        "id": "0XLf6_9RmSpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "  model_name: str\n",
        "\n",
        "  def setup(self):\n",
        "    self.model = FlaxAutoModel.from_pretrained(self.model_name)\n",
        "    self.target_token_idx = 0\n",
        "\n",
        "  def __call__(self, input_ids, attention_mask):\n",
        "    output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    last_hidden_state = output.last_hidden_state\n",
        "    return last_hidden_state[:, self.target_token_idx, :]"
      ],
      "metadata": {
        "id": "sh_35cIHkaNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Projection Head"
      ],
      "metadata": {
        "id": "4RXuFxEFydaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionHead(nn.Module):\n",
        "  projection_dim: int\n",
        "  dropout: float\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, train=True):\n",
        "    projected = nn.Dense(self.projection_dim)(x)\n",
        "    x = nn.gelu(projected)\n",
        "    x = nn.Dense(self.projection_dim)(x)\n",
        "    x = nn.Dropout(self.dropout, deterministic=not train)(x)\n",
        "    x += projected\n",
        "    return nn.LayerNorm()(x)"
      ],
      "metadata": {
        "id": "c5PT-D06yemQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLIP model"
      ],
      "metadata": {
        "id": "PHzKYJZbXGv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp"
      ],
      "metadata": {
        "id": "VPV-TeM2BB4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPDualEncoderModel(nn.Module):\n",
        "  image_encoder_alias: str\n",
        "  text_encoder_alias: str\n",
        "  projection_dims: int = 256\n",
        "  dropout: float = 0.1\n",
        "  temperature: float = 1.0\n",
        "\n",
        "  def setup(self):\n",
        "    self.image_encoder = ImageEncoder(\n",
        "        model_name=self.image_encoder_alias\n",
        "    )\n",
        "    self.text_encoder = TextEncoder(\n",
        "        model_name=self.text_encoder_alias\n",
        "    )\n",
        "    self.image_projection = ProjectionHead(\n",
        "        projection_dim=self.projection_dims,\n",
        "        dropout=self.dropout\n",
        "    )\n",
        "    self.text_projection = ProjectionHead(\n",
        "        projection_dim=self.projection_dims,\n",
        "        dropout=self.dropout\n",
        "    )\n",
        "\n",
        "  def __call__(self, inputs_image, inputs_input_ids, inputs_attention_mask, train=True):\n",
        "    i_e = self.get_image_features(inputs_image, train)\n",
        "    t_e = self.get_text_features(inputs_input_ids, inputs_attention_mask, train)\n",
        "    logits = jnp.dot(i_e, t_e.T) / self.temperature\n",
        "    return logits\n",
        "\n",
        "  def get_text_features(self, inputs_input_ids, inputs_attention_mask, train=False):\n",
        "    text_features = self.text_encoder(\n",
        "        input_ids=inputs_input_ids, attention_mask=inputs_attention_mask\n",
        "    )\n",
        "    text_embeddings = self.text_projection(text_features, train=train)\n",
        "    t_e = text_embeddings / jnp.linalg.norm(text_embeddings, axis=-1, keepdims=True)\n",
        "    return t_e\n",
        "\n",
        "  def get_image_features(self, inputs_image, train=False):\n",
        "    tmp_feat = self.image_encoder(inputs_image).pooler_output\n",
        "    # (batch_size, hidden_size, 1, 1) -> (batch_size, hidden_size)\n",
        "    image_features = tmp_feat.reshape((tmp_feat.shape[0], tmp_feat.shape[1]))\n",
        "    image_embeddings = self.image_projection(image_features, train=train)\n",
        "    i_e = image_embeddings / jnp.linalg.norm(image_embeddings, axis=-1, keepdims=True)\n",
        "    return i_e"
      ],
      "metadata": {
        "id": "tZYNEAAmXHwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Dataset"
      ],
      "metadata": {
        "id": "D4NP9OxPc4wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import abstractmethod\n",
        "class ImageRetrievalDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, artifact_id, tokenizer=None, target_size=None, max_length=200, lazy_loading=False):\n",
        "    super().__init__()\n",
        "    self.artifact_id = artifact_id\n",
        "    self.target_size = target_size\n",
        "    self.max_length = max_length\n",
        "    self.lazy_loading = lazy_loading\n",
        "    self.image_files, self.captions = self.fetch_dataset()\n",
        "    self.images = self.image_files\n",
        "\n",
        "    assert tokenizer is not None\n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "    self.tokenized_captions = tokenizer(\n",
        "        list(self.captions), padding=True, truncation=True,\n",
        "        max_length=self.max_length, return_tensors='pt'\n",
        "    )\n",
        "    self.transforms = A.Compose([\n",
        "        A.Resize(target_size, target_size, always_apply=True),\n",
        "        A.Normalize(max_pixel_value=255.0, always_apply=True)\n",
        "    ])\n",
        "\n",
        "  @abstractmethod\n",
        "  def fetch_dataset():\n",
        "    pass\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.captions)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    item = {\n",
        "        key: values[index]\n",
        "        for key, values in self.tokenized_captions.items()\n",
        "    }\n",
        "    image = cv2.imread(self.image_files[index])\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = self.transforms(image=image)[\"image\"]\n",
        "    item[\"image\"] = torch.tensor(image).permute(2, 0, 1).float()\n",
        "    item[\"caption\"] = self.captions[index]\n",
        "    return item"
      ],
      "metadata": {
        "id": "hOdsKsFpc6KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Filckr8kDataset(ImageRetrievalDataset):\n",
        "  def __init__(self, artifact_id, tokenizer=None, target_size=None, max_length=100, lazy_loading=False):\n",
        "    super().__init__(artifact_id, tokenizer, target_size, max_length, lazy_loading)\n",
        "\n",
        "  def fetch_dataset(self):\n",
        "    if wandb.run is None:\n",
        "      api = wandb.Api()\n",
        "      artifact = api.artifact(self.artifact_id, type=\"dataset\")\n",
        "    else:\n",
        "      articact = wandb.use_artifact(self.artifact_id, type=\"dataset\")\n",
        "\n",
        "    artifact_dir = artifact.download()\n",
        "    annotations = pd.read_csv(os.path.join(artifact_dir, \"captions.txt\"))\n",
        "    image_files = [\n",
        "        os.path.join(artifact_dir, \"Images\", image_file)\n",
        "        for image_file in annotations[\"image\"].to_list()\n",
        "    ]\n",
        "    for image_file in image_files:\n",
        "      assert os.path.isfile(image_file)\n",
        "    captions = annotations[\"caption\"].to_list()\n",
        "    return image_files, captions"
      ],
      "metadata": {
        "id": "wn-wI0Vzc_vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def numpy_collate(batch):\n",
        "  items = {}\n",
        "  for i, item in enumerate(batch):\n",
        "    for key, item in item.items():\n",
        "      item = item if key == 'caption' else np.array(item)\n",
        "      if not key in items:\n",
        "        items[key] = [item]\n",
        "      else:\n",
        "        items[key].append(item)\n",
        "  return {key: np.array(item) for key, item in items.items()}"
      ],
      "metadata": {
        "id": "-jLGOjhN2iaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "DATASET_LOOKUP = {\n",
        "    \"flickr8k\":  Filckr8kDataset\n",
        "}\n",
        "\n",
        "class ImageRetrievalDataModule:\n",
        "  def __init__(\n",
        "      self,\n",
        "      artifact_id: str,\n",
        "      dataset_name: str, \n",
        "      val_split: float = 0.2,\n",
        "      tokenizer_alias: Optional[str] = None,\n",
        "      target_size: int = 224,\n",
        "      max_length: int = 100,\n",
        "      lazy_loading: bool = False,\n",
        "      train_batch_size: int = 16,\n",
        "      val_batch_size: int = 16,\n",
        "      num_workers: int = 4,\n",
        "  ):\n",
        "    self.artifact_id = artifact_id\n",
        "    self.dataset_name = dataset_name\n",
        "    self.val_split = val_split\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_alias)\n",
        "    self.target_size = target_size\n",
        "    self.max_length = max_length\n",
        "    self.lazy_loading = lazy_loading\n",
        "    self.train_batch_size = train_batch_size\n",
        "    self.val_batch_size = val_batch_size\n",
        "    self.num_workers = num_workers\n",
        "    self.setup()\n",
        "\n",
        "  @staticmethod\n",
        "  def split_data(dataset: ImageRetrievalDataset, val_split: float):\n",
        "    train_length = int((1 - val_split) * len(dataset))\n",
        "    val_length = len(dataset) - train_length\n",
        "    train_dataset, val_dataset = random_split(\n",
        "        dataset, lengths=[train_length, val_length]\n",
        "    )\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "  def setup(\n",
        "      self,\n",
        "      stage: Optional[str] = None,\n",
        "  ) -> None:\n",
        "    dataset = DATASET_LOOKUP[self.dataset_name](\n",
        "        artifact_id=self.artifact_id,\n",
        "        tokenizer=self.tokenizer,\n",
        "        target_size=self.target_size,\n",
        "        max_length=self.max_length,\n",
        "        lazy_loading=self.lazy_loading,\n",
        "    )\n",
        "    self.train_dataset, self.val_dataset = self.split_data(dataset, val_split=self.val_split)\n",
        "    \n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(\n",
        "        self.train_dataset,\n",
        "        batch_size=self.train_batch_size,\n",
        "        num_workers=self.num_workers,\n",
        "        collate_fn=numpy_collate\n",
        "    )\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(\n",
        "        self.val_dataset,\n",
        "        batch_size=self.val_batch_size,\n",
        "        num_workers=self.num_workers,\n",
        "        collate_fn=numpy_collate\n",
        "    )"
      ],
      "metadata": {
        "id": "h_2pex5Rhq5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_encoder_alias = \"distilbert-base-uncased\"\n",
        "\n",
        "data_module = ImageRetrievalDataModule(\n",
        "    artifact_id=\"wandb/clip.lightning-image_retrieval/flickr-8k:latest\",\n",
        "    dataset_name=\"flickr8k\",\n",
        "    tokenizer_alias=text_encoder_alias,\n",
        "    lazy_loading=True\n",
        ")\n",
        "train_loader = data_module.train_dataloader()\n",
        "val_loader = data_module.val_dataloader()"
      ],
      "metadata": {
        "id": "wQMfkYaTkfBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'train: {len(train_loader)}, val: {len(val_loader)}')"
      ],
      "metadata": {
        "id": "DazaaIW5mOo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model init"
      ],
      "metadata": {
        "id": "2IuKAM801xPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flax.training import train_state, checkpoints\n",
        "import optax\n",
        "from jax import random\n",
        "import jax\n",
        "from flax import struct"
      ],
      "metadata": {
        "id": "SCETHB9Q3Jh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_inputs = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "btlYCeMbxXo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dummy_inputs.keys())\n",
        "print(type(dummy_inputs['image']))\n",
        "print(type(dummy_inputs['input_ids']))\n",
        "print(type(dummy_inputs['caption']))"
      ],
      "metadata": {
        "id": "VRewh3uA3VXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dummy_inputs['image'].shape)"
      ],
      "metadata": {
        "id": "3FTX6T_zVn_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@struct.dataclass\n",
        "class Metrics(metrics.Collection):\n",
        "  accuracy: metrics.Accuracy\n",
        "  loss: metrics.Average.from_output('loss')"
      ],
      "metadata": {
        "id": "m5AKyc2jwMRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainState(train_state.TrainState):\n",
        "  metrics: Metrics\n",
        "  key: jax.random.KeyArray"
      ],
      "metadata": {
        "id": "OpsZQKIkTWV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_encoder_alias = \"microsoft/resnet-50\"\n",
        "#image_encoder_alias = \"google/vit-base-patch16-224-in21k\"\n",
        "model = CLIPDualEncoderModel(image_encoder_alias, text_encoder_alias)\n",
        "main_rng = random.PRNGKey(42)\n",
        "main_rng, init_rng, dropout_rng = random.split(main_rng, 3)\n",
        "params = model.init(init_rng, dummy_inputs['image'], dummy_inputs['input_ids'], dummy_inputs['attention_mask'], train=False)['params']\n",
        "state = TrainState.create(apply_fn=model.apply, \n",
        "                          params=params,\n",
        "                          tx=optax.adam(1e-3),\n",
        "                          key=dropout_rng,\n",
        "                          metrics=Metrics.empty())"
      ],
      "metadata": {
        "id": "yBC_cvi41yWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jax.tree_map(lambda x: x.shape, params)"
      ],
      "metadata": {
        "id": "pkMvugt7MDgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "z598SZA2S5xS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "8oqvJjaOdY7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def train_step(state, inputs_image, inputs_input_ids, inputs_attention_mask, rng):\n",
        "  rng, new_dropout_rng = jax.random.split(rng)\n",
        "\n",
        "  def loss_fn(params):\n",
        "    logits = state.apply_fn(\n",
        "        {'params': params}, inputs_image, inputs_input_ids, inputs_attention_mask,\n",
        "        rngs={'dropout': new_dropout_rng}\n",
        "    )\n",
        "    labels = jnp.arange(logits.shape[0])\n",
        "    image_loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=labels)\n",
        "    text_loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits.T, labels=labels)\n",
        "    loss = (image_loss + text_loss) / 2.0\n",
        "    return loss.mean()\n",
        "\n",
        "  grad_fn = jax.value_and_grad(loss_fn)\n",
        "  loss, grads = grad_fn(state.params)\n",
        "  state = state.apply_gradients(grads=grads)\n",
        "  return state, loss, rng"
      ],
      "metadata": {
        "id": "06BnDk0aORmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(state, train_loader, rng):\n",
        "  train_losses = []\n",
        "  for batch in tqdm(train_loader, leave=False):\n",
        "    inputs_image = batch['image']\n",
        "    inputs_input_ids = batch['input_ids']\n",
        "    inputs_attention_mask = batch['attention_mask']\n",
        "    state, loss, rng = train_step(state, inputs_image, inputs_input_ids, inputs_attention_mask, rng)\n",
        "    train_losses.append(loss)\n",
        "  return state, train_losses, rng"
      ],
      "metadata": {
        "id": "vO59JE3VdIJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def eval_step(state, inputs_image, inputs_input_ids, inputs_attention_mask):\n",
        "  logits = state.apply_fn(\n",
        "      {'params': state.params}, inputs_image, inputs_input_ids, inputs_attention_mask, train=False\n",
        "  )\n",
        "  labels = jnp.arange(logits.shape[0])\n",
        "  image_loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=labels)\n",
        "  text_loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits.T, labels=labels)\n",
        "  loss = (image_loss + text_loss) / 2.0\n",
        "  return loss.mean()"
      ],
      "metadata": {
        "id": "biMhyM1Xv19E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_epoch(state, val_loader):\n",
        "  val_losses = []\n",
        "  for batch in tqdm(val_loader, leave=False):\n",
        "    inputs_image = batch['image']\n",
        "    inputs_input_ids = batch['input_ids']\n",
        "    inputs_attention_mask = batch['attention_mask']\n",
        "    loss = eval_step(state, inputs_image, inputs_input_ids, inputs_attention_mask)\n",
        "    val_losses.append(loss)\n",
        "  return val_losses"
      ],
      "metadata": {
        "id": "hx5YiBDxvgem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(state, train_loader, val_loader, rng, num_epochs=20, ckpt_dir='tmp/flax-checkpointing'):\n",
        "  metrics_history = {\n",
        "      'train_loss': [],\n",
        "      'train_accuracy': [],\n",
        "      'val_loss': [],\n",
        "      'val_accuracy': []\n",
        "  }\n",
        "\n",
        "  for epoch_idx in tqdm(range(1, num_epochs + 1), leave=False):\n",
        "    # Run optimization steps over training batches and compute batch metrics\n",
        "    state, train_losses, rng = train_epoch(state, train_loader, rng)\n",
        "    metrics_history['train_loss'].extend(train_losses)\n",
        "    \n",
        "    eval_losses = eval_epoch(state, val_loader)\n",
        "    metrics_history['val_loss'].extend(eval_losses)\n",
        "\n",
        "    print(f\"epoch: {epoch_idx} | \"\n",
        "          f\"train loss: {metrics_history['train_loss'][-1]}, \"\n",
        "          f\"val loss: {metrics_history['val_loss'][-1]}\"\n",
        "    )\n",
        "\n",
        "  checkpoints.save_checkpoint(\n",
        "      ckpt_dir=ckpt_dir,\n",
        "      target=state,\n",
        "      step=0\n",
        "  )\n",
        "\n",
        "  return state"
      ],
      "metadata": {
        "id": "NWyn9CEAia6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = train_model(state, train_loader, val_loader, main_rng, num_epochs=20)"
      ],
      "metadata": {
        "id": "UZM5aTe_dpH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find matches"
      ],
      "metadata": {
        "id": "dvGzZSOVWBB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_embeds(state, val_loader):\n",
        "  image_embeds = []\n",
        "  for batch in tqdm(val_loader):\n",
        "    inputs_image = batch['image']\n",
        "    inputs_input_ids = batch['input_ids']\n",
        "    inputs_attention_mask = batch['attention_mask']\n",
        "    i_e, t_e = state.apply_fn(\n",
        "        {'params': state.params}, inputs_image, inputs_input_ids, inputs_attention_mask, train=False\n",
        "    )\n",
        "    image_embeds.append(i_e)\n",
        "  return image_embeds"
      ],
      "metadata": {
        "id": "h_mRqaLnWExY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model from sample"
      ],
      "metadata": {
        "id": "2t-EG34weUNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(logits, axis):\n",
        "  logprobs = jax.nn.log_softmax(logits, axis=axis)\n",
        "  nll = jnp.diag(logprobs)\n",
        "  ce = -jnp.mean(nll)\n",
        "  return ce"
      ],
      "metadata": {
        "id": "9rGHbWuq1mV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_loss(similarity):\n",
        "  loss = (\n",
        "      cross_entropy(similarity, axis=0) + cross_entropy(similarity, axis=1)\n",
        "  ) / 2\n",
        "  return loss"
      ],
      "metadata": {
        "id": "b4-AeHi71c1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def train_step_sample(state, inputs_image, inputs_input_ids, inputs_attention_mask):\n",
        "  dropout_rng, new_dropout_rng = jax.random.split(state.key)\n",
        "\n",
        "  def compute_loss(params):\n",
        "    logits = state.apply_fn(\n",
        "        {'params': params}, inputs_image, inputs_input_ids, inputs_attention_mask,\n",
        "        rngs={'dropout': dropout_rng}\n",
        "    )\n",
        "    loss = clip_loss(logits)\n",
        "    return loss\n",
        "\n",
        "  grad_fn = jax.value_and_grad(compute_loss)\n",
        "  loss, grad = grad_fn(state.params)\n",
        "  new_state = state.apply_gradients(grads=grad)\n",
        "  metrics = {\n",
        "      'loss': loss\n",
        "  }\n",
        "  return new_state, metrics"
      ],
      "metadata": {
        "id": "lllGlUpxeXIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def eval_step_sample(state, inputs_image, inputs_input_ids, inputs_attention_mask):\n",
        "  logits = state.apply_fn(\n",
        "      {'params': state.params}, inputs_image, inputs_input_ids, inputs_attention_mask, train=False\n",
        "  )\n",
        "  loss = clip_loss(logits)\n",
        "  metrics = {'loss': loss}\n",
        "  return metrics"
      ],
      "metadata": {
        "id": "6fXBb2AQWgka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_sample(state, train_loader, val_loader, num_epochs=20, ckpt_dir='tmp/flax-checkpointing'):\n",
        "  for epoch_idx in tqdm(range(1, num_epochs + 1), leave=False):\n",
        "    train_metrics = []\n",
        "    for batch in tqdm(train_loader, leave=False):\n",
        "      inputs_image = batch['image']\n",
        "      inputs_input_ids = batch['input_ids']\n",
        "      inputs_attention_mask = batch['attention_mask']\n",
        "      state, metrics = train_step_sample(state, inputs_image, inputs_input_ids, inputs_attention_mask)\n",
        "      train_metrics.append(metrics)\n",
        "    print(f\"Epoch... ({epoch_idx} | Train Loss: {train_metrics[-1]['loss']}\")\n",
        "\n",
        "    eval_metrics = []\n",
        "    for batch in tqdm(val_loader, leave=False):\n",
        "      inputs_image = batch['image']\n",
        "      inputs_input_ids = batch['input_ids']\n",
        "      inputs_attention_mask = batch['attention_mask']\n",
        "      metrics = eval_step_sample(state, inputs_image, inputs_input_ids, inputs_attention_mask)\n",
        "      eval_metrics.append(metrics)\n",
        "    print(f\"Epoch... ({epoch_idx} | Eval Loss: {eval_metrics[-1]['loss']}\")"
      ],
      "metadata": {
        "id": "Dtt2UT9g3CJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_model_sample(state, train_loader, val_loader, num_epochs=20)"
      ],
      "metadata": {
        "id": "62JmmQ6s4buy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}