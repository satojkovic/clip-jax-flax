{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9dqV58hM2NN"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers\n",
        "!pip install git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "id": "gVgSnr2FNADw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/Lightning-AI/lightning.git"
      ],
      "metadata": {
        "id": "taYkRquANBMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations"
      ],
      "metadata": {
        "id": "Sf9jgWTiNIUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "import wandb\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "pykSb_W3NDna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightning"
      ],
      "metadata": {
        "id": "IPrB78wjNGMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A"
      ],
      "metadata": {
        "id": "sIhQhu2lNHkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image encoder\n",
        "\n",
        "Use FlaxResNetModel from huggingface transformers"
      ],
      "metadata": {
        "id": "aq81mz1_ePYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoImageProcessor, FlaxResNetModel"
      ],
      "metadata": {
        "id": "cBnzAXGsePGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flax import linen as nn"
      ],
      "metadata": {
        "id": "Mvrk2jbPxYW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "  model_name: str\n",
        "  pretrained: bool = True\n",
        "  trainable: bool = True\n",
        "\n",
        "  def setup(self):\n",
        "    self.model = FlaxResNetModel.from_pretrained(self.model_name)\n",
        "    self.image_processor = AutoImageProcessor.from_pretrained(self.model_name)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    inputs = self.image_processor(images=x, return_tensors='np')\n",
        "    outputs = self.model(**inputs)\n",
        "    return outputs.pooler_output"
      ],
      "metadata": {
        "id": "dltZZaDdy2uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Encoder"
      ],
      "metadata": {
        "id": "aAFyZIDbkZNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import FlaxAutoModel"
      ],
      "metadata": {
        "id": "0XLf6_9RmSpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "  model_name: str\n",
        "  trainable: bool = True\n",
        "\n",
        "  def setup(self):\n",
        "    self.model = FlaxAutoModel.from_pretrained(self.model_name)\n",
        "    self.target_token_idx = 0\n",
        "\n",
        "  def __call__(self, input_ids, attention_mask):\n",
        "    output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    last_hidden_state = output.last_hidden_state\n",
        "    return last_hidden_state[:, self.target_token_idx, :]"
      ],
      "metadata": {
        "id": "sh_35cIHkaNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Projection Head"
      ],
      "metadata": {
        "id": "4RXuFxEFydaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionHead(nn.Module):\n",
        "  embedding_dim: int\n",
        "  projection_dim: int\n",
        "  dropout: float\n",
        "\n",
        "  def setup(self):\n",
        "    self.projection = nn.Dense(self.projection_dim)\n",
        "    self.gelu = nn.gelu()\n",
        "    self.fc = nn.Dense(self.projection_dim)\n",
        "    self.dropout = nn.Dropout(self.dropout)\n",
        "    self.layer_norm = nn.LayerNorm()\n",
        "\n",
        "  def __call__(self, x):\n",
        "    projected = self.projection(x)\n",
        "    x = self.gelu(projected)\n",
        "    x = self.fc(x)\n",
        "    x = self.dropout(x)\n",
        "    x += projected\n",
        "    return self.layer_norm(x)"
      ],
      "metadata": {
        "id": "c5PT-D06yemQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLIP model"
      ],
      "metadata": {
        "id": "PHzKYJZbXGv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPDualEncoderModel(nn.Module):\n",
        "  image_encoder_alias: str\n",
        "  text_encoder_alias: str\n",
        "  image_encoder_pretrained: bool = True\n",
        "  image_encoder_trainable: bool = True\n",
        "  text_encoder_trainable: bool = True\n",
        "  image_embedding_dims: int = 2048\n",
        "  text_embedding_dims: int = 768\n",
        "  projection_dims: int = 256\n",
        "  dropout: float = 0.0\n",
        "\n",
        "  def setup(self):\n",
        "    self.image_encoder = ImageEncoder(\n",
        "        model_name=self.image_encoder_alias,\n",
        "        pretrained=self.image_encoder_pretrained,\n",
        "        trainable=self.image_encoder_trainable\n",
        "    )\n",
        "    self.text_encoder = TextEncoder(\n",
        "        model_name=self.text_encoder_alias,\n",
        "        trainable=self.text_encoder_trainable\n",
        "    )\n",
        "    self.image_projection = ProjectionHead(\n",
        "        embedding_dim=self.image_embedding_dims,\n",
        "        projection_dim=self.projection_dims,\n",
        "        dropout=self.dropout\n",
        "    )\n",
        "    self.text_projection = ProjectionHead(\n",
        "        embedding_dim=self.text_embedding_dims,\n",
        "        projection_dim=self.projection_dims,\n",
        "        dropout=self.dropout\n",
        "    )\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    image_features = self.image_encoder(inputs['image'])\n",
        "    text_features = self.text_encoder(\n",
        "        input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask']\n",
        "    )\n",
        "\n",
        "    image_embeddings = self.image_projection(image_features)\n",
        "    text_embeddings = self.text_projection(text_features)\n",
        "\n",
        "    return image_embeddings, text_embeddings"
      ],
      "metadata": {
        "id": "tZYNEAAmXHwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Dataset"
      ],
      "metadata": {
        "id": "D4NP9OxPc4wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import abstractmethod\n",
        "class ImageRetrievalDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, artifact_id, tokenizer=None, target_size=None, max_length=200, lazy_loading=False):\n",
        "    super().__init__()\n",
        "    self.artifact_id = artifact_id\n",
        "    self.target_size = target_size\n",
        "    self.max_length = max_length\n",
        "    self.lazy_loading = lazy_loading\n",
        "    self.image_files, self.captions = self.fetch_dataset()\n",
        "    self.images = self.image_files\n",
        "\n",
        "    assert tokenizer is not None\n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "    self.tokenized_captions = tokenizer(\n",
        "        list(self.captions), padding=True, truncation=True,\n",
        "        max_length=self.max_length, return_tensors='pt'\n",
        "    )\n",
        "    self.transforms = A.Compose([\n",
        "        A.Resize(target_size, target_size, always_apply=True),\n",
        "        A.Normalize(max_pixel_value=255.0, always_apply=True)\n",
        "    ])\n",
        "\n",
        "  @abstractmethod\n",
        "  def fetch_dataset():\n",
        "    pass\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.captions)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    item = {\n",
        "        key: values[index]\n",
        "        for key, values in self.tokenized_captions.items()\n",
        "    }\n",
        "    image = cv2.imread(self.image_files[index])\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = self.transforms(image=image)[\"image\"]\n",
        "    item[\"image\"] = torch.tensor(image).permute(2, 0, 1).float()\n",
        "    item[\"caption\"] = self.captions[index]\n",
        "    return item"
      ],
      "metadata": {
        "id": "hOdsKsFpc6KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Filckr8kDataset(ImageRetrievalDataset):\n",
        "  def __init__(self, artifact_id, tokenizer=None, target_size=None, max_length=100, lazy_loading=False):\n",
        "    super().__init__(artifact_id, tokenizer, target_size, max_length, lazy_loading)\n",
        "\n",
        "  def fetch_dataset(self):\n",
        "    if wandb.run is None:\n",
        "      api = wandb.Api()\n",
        "      artifact = api.artifact(self.artifact_id, type=\"dataset\")\n",
        "    else:\n",
        "      articact = wandb.use_artifact(self.artifact_id, type=\"dataset\")\n",
        "\n",
        "    artifact_dir = artifact.download()\n",
        "    annotations = pd.read_csv(os.path.join(artifact_dir, \"captions.txt\"))\n",
        "    image_files = [\n",
        "        os.path.join(artifact_dir, \"Images\", image_file)\n",
        "        for image_file in annotations[\"image\"].to_list()\n",
        "    ]\n",
        "    for image_file in image_files:\n",
        "      assert os.path.isfile(image_file)\n",
        "    captions = annotations[\"caption\"].to_list()\n",
        "    return image_files, captions"
      ],
      "metadata": {
        "id": "wn-wI0Vzc_vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "#from pytorch_lightning import LightningDataModule # pygments error\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "DATASET_LOOKUP = {\n",
        "    \"flickr8k\":  Filckr8kDataset\n",
        "}\n",
        "\n",
        "class ImageRetrievalDataModule:\n",
        "  def __init__(\n",
        "      self,\n",
        "      artifact_id: str,\n",
        "      dataset_name: str, \n",
        "      val_split: float = 0.2,\n",
        "      tokenizer_alias: Optional[str] = None,\n",
        "      target_size: int = 224,\n",
        "      max_length: int = 100,\n",
        "      lazy_loading: bool = False,\n",
        "      train_batch_size: int = 16,\n",
        "      val_batch_size: int = 16,\n",
        "      num_workers: int = 4,\n",
        "  ):\n",
        "    self.artifact_id = artifact_id\n",
        "    self.dataset_name = dataset_name\n",
        "    self.val_split = val_split\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_alias)\n",
        "    self.target_size = target_size\n",
        "    self.max_length = max_length\n",
        "    self.lazy_loading = lazy_loading\n",
        "    self.train_batch_size = train_batch_size\n",
        "    self.val_batch_size = val_batch_size\n",
        "    self.num_workers = num_workers\n",
        "    self.setup()\n",
        "\n",
        "  @staticmethod\n",
        "  def split_data(dataset: ImageRetrievalDataset, val_split: float):\n",
        "    train_length = int((1 - val_split) * len(dataset))\n",
        "    val_length = len(dataset) - train_length\n",
        "    train_dataset, val_dataset = random_split(\n",
        "        dataset, lengths=[train_length, val_length]\n",
        "    )\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "  def setup(\n",
        "      self,\n",
        "      stage: Optional[str] = None,\n",
        "  ) -> None:\n",
        "    dataset = DATASET_LOOKUP[self.dataset_name](\n",
        "        artifact_id=self.artifact_id,\n",
        "        tokenizer=self.tokenizer,\n",
        "        target_size=self.target_size,\n",
        "        max_length=self.max_length,\n",
        "        lazy_loading=self.lazy_loading,\n",
        "    )\n",
        "    self.train_dataset, self.val_dataset = self.split_data(dataset, val_split=self.val_split)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(\n",
        "        self.train_dataset,\n",
        "        batch_size=self.train_batch_size,\n",
        "        num_workers=self.num_workers\n",
        "    )\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(\n",
        "        self.val_dataset,\n",
        "        batch_size=self.val_batch_size,\n",
        "        num_workers=self.num_workers,\n",
        "    )"
      ],
      "metadata": {
        "id": "h_2pex5Rhq5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_encoder_alias = \"distilbert-base-uncased\"\n",
        "\n",
        "data_module = ImageRetrievalDataModule(\n",
        "    artifact_id=\"wandb/clip.lightning-image_retrieval/flickr-8k:latest\",\n",
        "    dataset_name=\"flickr8k\",\n",
        "    tokenizer_alias=text_encoder_alias,\n",
        "    lazy_loading=True\n",
        ")\n",
        "train_loader = data_module.train_dataloader()\n",
        "val_loader = data_module.val_dataloader()"
      ],
      "metadata": {
        "id": "wQMfkYaTkfBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'train: {len(train_loader)}, val: {len(val_loader)}')"
      ],
      "metadata": {
        "id": "DazaaIW5mOo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model init"
      ],
      "metadata": {
        "id": "2IuKAM801xPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flax.training import train_state, checkpoints\n",
        "import optax"
      ],
      "metadata": {
        "id": "SCETHB9Q3Jh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_encoder_alias = \"resnet50\"\n",
        "model = CLIPDualEncoderModel(image_encoder_alias, text_encoder_alias)\n",
        "state = train_state.TrainState.create(apply_fn=model.__call__, \n",
        "                                      params=model.params, \n",
        "                                      tx=optax.adam(1e-3),\n",
        "                                      )"
      ],
      "metadata": {
        "id": "yBC_cvi41yWT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}