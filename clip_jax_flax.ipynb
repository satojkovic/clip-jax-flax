{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "b9oWlBnpb9Dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlVdD78PCaz8"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "# Installs the wheel compatible with CUDA 11 and cuDNN 8.2 or newer.\n",
        "# Note: wheels only available on linux.\n",
        "!pip install --upgrade \"jax[cuda]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flax -U\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "CsDH-fIjCnBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jax setup for colab"
      ],
      "metadata": {
        "id": "bUOsgbFBThfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/google/flax/issues/2263#issuecomment-1173424293\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "  import jax.tools.colab_tpu\n",
        "  jax.tools.colab_tpu.setup_tpu()"
      ],
      "metadata": {
        "id": "v6w1d-63ThC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset download\n",
        "https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/"
      ],
      "metadata": {
        "id": "W7a-hZSKcrJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip"
      ],
      "metadata": {
        "id": "XU6GsgFpCyUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip"
      ],
      "metadata": {
        "id": "sAf8igWsch9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Flickr8k_Dataset.zip"
      ],
      "metadata": {
        "id": "tmC91GQMcv5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Flickr8k_text.zip"
      ],
      "metadata": {
        "id": "tvOmtmAVcz6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "9MD4drVHWRIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "g-Ry9l_7FdS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-image"
      ],
      "metadata": {
        "id": "MGmgWg0cHFv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import cv2\n",
        "from transformers import AutoTokenizer\n",
        "from skimage import io\n",
        "import os\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "0yEhegGTWSKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub"
      ],
      "metadata": {
        "id": "Fw-hNn-JXyFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_encoder = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
      ],
      "metadata": {
        "id": "Esyn8QX6YEnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\"I am a sentence for which I would like to get its embedding.\"]"
      ],
      "metadata": {
        "id": "89ueqH1aZIm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed = sentence_encoder(message)"
      ],
      "metadata": {
        "id": "pb6awyhJZM-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'embed: {embed.shape}')\n",
        "print(f'type: {type(embed)}')"
      ],
      "metadata": {
        "id": "ZjS54Y7tRaQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_np = np.array(embed)"
      ],
      "metadata": {
        "id": "KcjeI5hIZV9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'type: {type(embed_np)}')"
      ],
      "metadata": {
        "id": "L0gO8FP1SFDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'embed: {\", \".join(map(str, list(np.squeeze(embed_np)[:3])))}, ...')"
      ],
      "metadata": {
        "id": "tKA0ZQpraGTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-process captions\n",
        "\n",
        "* lower letters\n",
        "* punctuation"
      ],
      "metadata": {
        "id": "XlxmPH6o5ZyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_captions_from_file(path):\n",
        "  captions = []\n",
        "  with open(path, 'r') as f:\n",
        "      for line in f:\n",
        "          line = line.strip()\n",
        "          elems = line.split('\\t')\n",
        "          fn_id = elems[0].split('#')  # [filename, id]\n",
        "          captions.append(fn_id + [elems[1].lower()])  # [[filaneme, id, caption], ...]\n",
        "  return captions"
      ],
      "metadata": {
        "id": "yBF5ckycWWjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "captions = load_captions_from_file('Flickr8k.token.txt')"
      ],
      "metadata": {
        "id": "eesC9A005sEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(captions[0])"
      ],
      "metadata": {
        "id": "kJyMi0E7wyWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_caption = pd.DataFrame(captions, columns=['image_filename', 'id', 'caption'])"
      ],
      "metadata": {
        "id": "8w5iJ5Q-58SW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Unique images: {len(np.unique(df_caption.image_filename.values))}')\n",
        "print(f'Total captions: {len(df_caption)}')"
      ],
      "metadata": {
        "id": "G8S8D5ewCAvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = str.maketrans('', '', string.punctuation)\n",
        "def remove_punctuation(text):\n",
        "  return text.translate(translator)"
      ],
      "metadata": {
        "id": "pWIeB47WMg9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = df_caption['caption'].iloc[0]\n",
        "print(f'original: {text}')\n",
        "print(f'removed: {remove_punctuation(text)}')"
      ],
      "metadata": {
        "id": "f66uOWqRNffj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "  cleaned_text = remove_punctuation(text)\n",
        "  return cleaned_text"
      ],
      "metadata": {
        "id": "0dAvLk4COlcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(df_caption):\n",
        "  for i, caption in enumerate(df_caption.caption.values):\n",
        "    cleaned_caption = clean_text(caption)\n",
        "    df_caption['caption'].iloc[i] = cleaned_caption\n",
        "  return df_caption"
      ],
      "metadata": {
        "id": "Es_HKBz9Os-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_caption_0 = df_caption.loc[df_caption['id'].values == '0', :]"
      ],
      "metadata": {
        "id": "z0oL9NZrQepc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_filenames = df_caption_0.image_filename.values\n",
        "captions = df_caption_0.caption.values"
      ],
      "metadata": {
        "id": "qNCw1Ap9RgoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max([len(c.split()) for c in captions])"
      ],
      "metadata": {
        "id": "1NoB-uclDl3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'max_length: {max_length}')"
      ],
      "metadata": {
        "id": "dvOuoyYmD3jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{image_filenames[0]}: {captions[0]}')"
      ],
      "metadata": {
        "id": "lVdZ8FnNSEE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flickr8k dataset"
      ],
      "metadata": {
        "id": "ZvZnOEGD8MIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flickr8kDataset:\n",
        "  def __init__(self, caption_text):\n",
        "    self.caption_text = caption_text\n",
        "    self.translator = str.maketrans('', '', string.punctuation)\n",
        "    self._setup()\n",
        "\n",
        "  def _setup(self):\n",
        "    self.captions = self._load_captions_from_file(self.caption_text)\n",
        "    self.df_caption = pd.DataFrame(self.captions, columns=['image_filename', 'id', 'caption'])\n",
        "    self.df_caption = self._preprocess(self.df_caption)\n",
        "    self.image_filenames, self.captions = self._extract(self.df_caption)\n",
        "    self.max_length = max([len(c.split()) for c in self.captions])\n",
        "\n",
        "  def _load_captions_from_file(self,caption_text):\n",
        "    captions = []\n",
        "    with open(caption_text, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            elems = line.split('\\t')\n",
        "            fn_id = elems[0].split('#')  # [filename, id]\n",
        "            captions.append(fn_id + [elems[1].lower()])  # [[filaneme, id, caption], ...]\n",
        "    return captions\n",
        "\n",
        "  def _remove_punctuation(self, text):\n",
        "    return text.translate(self.translator)\n",
        "  \n",
        "  def _clean_text(self, text):\n",
        "    cleaned_text = self._remove_punctuation(text)\n",
        "    return cleaned_text\n",
        "\n",
        "  def _preprocess(self, df_caption):\n",
        "    for i, caption in enumerate(df_caption.caption.values):\n",
        "      cleaned_caption = self._clean_text(caption)\n",
        "      df_caption['caption'].iloc[i] = cleaned_caption\n",
        "    return df_caption\n",
        "\n",
        "  def _extract(self, df_caption):\n",
        "    df_caption_0 = df_caption.loc[df_caption['id'].values == '0', :]\n",
        "    image_filenames = df_caption_0.image_filename.values\n",
        "    captions = df_caption_0.caption.values\n",
        "    return image_filenames, captions"
      ],
      "metadata": {
        "id": "_uXN_leP8LeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flickr8k_dataset = Flickr8kDataset('Flickr8k.token.txt')"
      ],
      "metadata": {
        "id": "XlgZ9bXb_Dtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'max_length: {flickr8k_dataset.max_length}')"
      ],
      "metadata": {
        "id": "xskeURLk_Trw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader"
      ],
      "metadata": {
        "id": "ESH67VNZUBB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, root_dir, image_files, captions, max_length, tokenizer=None, transforms=None):\n",
        "    self.image_files = [os.path.join(root_dir, f) for f in image_files]\n",
        "    self.captions = captions\n",
        "    self.max_length = max_length\n",
        "    self.tokenizer = tokenizer if tokenizer is not None else AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    image_file, caption = self.image_files[index], self.captions[index]\n",
        "    tokens = self.tokenizer(caption, truncation=True, padding=\"max_length\", max_length=self.max_length)\n",
        "    image = io.imread(image_file)\n",
        "    if self.transforms:\n",
        "      image = self.transforms(image)\n",
        "    return image, tokens\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_files)"
      ],
      "metadata": {
        "id": "SyTE_t91c7QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip_dataset = CLIPDataset(\n",
        "    root_dir='Flicker8k_Dataset',\n",
        "    image_files=image_filenames,\n",
        "    captions=captions,\n",
        "    max_length=max_length,\n",
        "    transforms=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "    ])\n",
        ")"
      ],
      "metadata": {
        "id": "rRFTTHvEJoij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image encoder"
      ],
      "metadata": {
        "id": "F6kCSZqtAK1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "id": "9yMCroDrALzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timm"
      ],
      "metadata": {
        "id": "fAwkC-nBCrpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = timm.create_model('resnet50', pretrained=True, num_classes=0)"
      ],
      "metadata": {
        "id": "NInurUCxCy8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "o = model.forward(torch.randn(2, 3, 299, 299))"
      ],
      "metadata": {
        "id": "d3cuiQuIC3A7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'resnet50 feature: {o.shape}')"
      ],
      "metadata": {
        "id": "C_ouQfGbDC9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageEncoder(torch.nn.Module):\n",
        "  def __init__(self, model_name, pretrained=True, num_classes=0):\n",
        "    super(ImageEncoder, self).__init__()\n",
        "    self.model = timm.create_model(\n",
        "        model_name=model_name, pretrained=pretrained, num_classes=num_classes\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)"
      ],
      "metadata": {
        "id": "BRis_9xVDmtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text encoder"
      ],
      "metadata": {
        "id": "QYDg3bDMQJ__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(TextEncoder, self).__init__()\n",
        "    self.sentence_encoder = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "\n",
        "  def forward(self, x):\n",
        "    embed_np = np.array(self.sentence_encoder(x))\n",
        "    return embed_np"
      ],
      "metadata": {
        "id": "XnRTheQCQLGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Projection head"
      ],
      "metadata": {
        "id": "3D8kG-AtzbDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionHead(torch.nn.Module):\n",
        "  def __init__(self, embed_dim, proj_dim, drop_ratio):\n",
        "    super(ProjectionHead, self).__init__()\n",
        "    self.embed_dim = embed_dim\n",
        "    self.proj = nn.Linear(embed_dim, proj_dim)\n",
        "    self.gelu = nn.GELU()\n",
        "    self.fc = nn.Linear(proj_dim, proj_dim)\n",
        "    self.dropout = nn.Dropout(drop_ratio)\n",
        "    self.layer_norm = nn.LayerNorm(proj_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_proj = self.proj(x)\n",
        "    x = self.gelu(x_proj)\n",
        "    x = self.fc(x)\n",
        "    x = self.dropout(x)\n",
        "    x = x + x_proj\n",
        "    x = self.layer_norm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "iQRXzqlJzdKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLIP model"
      ],
      "metadata": {
        "id": "wOkbj1s4ejz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIP(torch.nn.Module):\n",
        "  def __init__(self, img_embed_dim, text_embed_dim, proj_dim, drop_ratio, temperature=1.0):\n",
        "    super(CLIP, self).__init__()\n",
        "    self.image_encoder = ImageEncoder('resnet50')\n",
        "    self.text_encoder = TextEncoder()\n",
        "    self.image_head = ProjectionHead(img_embed_dim, proj_dim, drop_ratio)\n",
        "    self.text_head = ProjectionHead(text_embed_dim, proj_dim, drop_ratio)\n",
        "    self.temperature = temperature\n",
        "\n",
        "  def forward(self, img, tokens):\n",
        "    assert img.shape[0] == tokens.shape[0]\n",
        "    i_f = self.image_encoder(img)\n",
        "    t_f = self.text_encoder(tokens)\n",
        "    i_e = self.image_head(i_f)\n",
        "    t_e = self.text_head(t_f)\n",
        "    logits = (i_e @ t_e.T) / self.temperature\n",
        "    return logits"
      ],
      "metadata": {
        "id": "JhwhPQ2ZekyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss function"
      ],
      "metadata": {
        "id": "EJFiFDXrbjr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(logits, loss_fn):\n",
        "  n = logits.shape[0]\n",
        "  labels = np.arange(n)\n",
        "  loss_i = loss_fn(torch.transpose(logits, 0, 1), labels)\n",
        "  loss_t = loss_fn(logits, labels)\n",
        "  return (loss_i + loss_t) / 2"
      ],
      "metadata": {
        "id": "93WPuwSObl0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss function"
      ],
      "metadata": {
        "id": "XwyeTt9Crb_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss_fn(name):\n",
        "  if name == 'cross_entropy':\n",
        "    return nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Bg2scbM5rds8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer"
      ],
      "metadata": {
        "id": "wM5IHNI8r0zC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optim(name, model):\n",
        "  if name == 'adam':\n",
        "    return torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "Dzvfv4A4r2Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train epoch"
      ],
      "metadata": {
        "id": "qL8n4L4KMiBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(train_loader, model, optimizer, loss_fn):\n",
        "  model.train()\n",
        "  running_loss = 0.\n",
        "  last_loss = 0.\n",
        "\n",
        "  for cur_iter, batch in enumerate(train_loader):\n",
        "    batch_image, batch_tokens = batch\n",
        "\n",
        "    # Zero your gradients for every batch\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Make predictions for this batch\n",
        "    logits = model(batch_image, batch_tokens)\n",
        "\n",
        "    # Compute the loss and gradients\n",
        "    loss = compute_loss(logits, loss_fn)\n",
        "    loss.backward()\n",
        "\n",
        "    # Adjust learning weight\n",
        "    optimizer.step()\n",
        "\n",
        "    # Gather data and report\n",
        "    running_loss += loss.item()\n",
        "    if cur_iter % 10 == 9:\n",
        "      last_loss = running_loss / 10\n",
        "      print(f'batch {cur_iter+1} loss: {last_loss}')\n",
        "  return last_loss"
      ],
      "metadata": {
        "id": "evobkSTfMjVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLIP parameters"
      ],
      "metadata": {
        "id": "HQVHa3sM-dPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_clip_cfg(\n",
        "    num_epochs=20,\n",
        "    img_embed_dim=2048,  # resnet50\n",
        "    text_embed_dim=512,  # universal sentence encoder\n",
        "    proj_dim=256,\n",
        "    drop_ratio=0.1,\n",
        "    temperature=1.0,\n",
        "):\n",
        "  cfg = {}\n",
        "  cfg['num_epochs'] = num_epochs\n",
        "  cfg['img_embed_dim'] = img_embed_dim\n",
        "  cfg['text_embed_dim'] = text_embed_dim\n",
        "  cfg['proj_dim'] = proj_dim\n",
        "  cfg['drop_ratio'] = drop_ratio\n",
        "  cfg['temperature'] = temperature\n",
        "\n",
        "  # dataset\n",
        "  cfg['root_dir'] = 'Flicker8k_Dataset'\n",
        "  cfg['token_text'] = 'Flickr8k.token.txt'\n",
        "\n",
        "  return cfg"
      ],
      "metadata": {
        "id": "1ghA-5AiC5B-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip_cfg = get_clip_cfg(num_epochs=20)"
      ],
      "metadata": {
        "id": "-VJrOW0xDpd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in clip_cfg.items():\n",
        "  print(f'{k}: {v}')"
      ],
      "metadata": {
        "id": "HgHNOz5HEHAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "9fgyUa3HD-aG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_dataset = CLIPDataset(\n",
        "    root_dir=cfg['root_dir'],\n",
        "    image_files=flickr8k_dataset.image_filenames,\n",
        "    captions=flickr8k_dataset.captions,\n",
        "    max_length=flickr8k_dataset.max_length,\n",
        "    transforms=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "    ])\n",
        ")"
      ],
      "metadata": {
        "id": "DIjC8y_Hg1mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(cfg, dataset):\n",
        "  model = CLIP(\n",
        "      cfg['img_embed_dim'], \n",
        "      cfg['text_embed_dim'],\n",
        "      cfg['proj_dim'],\n",
        "      cfg['drop_ratio'],\n",
        "      cfg['temperature']\n",
        "  )"
      ],
      "metadata": {
        "id": "9byL0-daQ3Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(clip_cfg, flickr8k_dataset)"
      ],
      "metadata": {
        "id": "MmRkOe3azRd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vykfSfCP1exz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}