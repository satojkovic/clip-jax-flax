{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset download\n",
        "https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/"
      ],
      "metadata": {
        "id": "3rCvdseu-oQw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lLhMKLvEX_7"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip"
      ],
      "metadata": {
        "id": "D_vCegby-pK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Flickr8k_Dataset.zip"
      ],
      "metadata": {
        "id": "NyU0wIKS-vga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Flickr8k_text.zip"
      ],
      "metadata": {
        "id": "Za7kxo3A-xeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "Iw2hMtA__fpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "cwcp-iyAUUlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers\n",
        "!pip install git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "id": "kDs7oa1eXrn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/Lightning-AI/lightning.git\n",
        "#!pip install git+https://github.com/Lightning-AI/lightning.git@bugfix/colab-import"
      ],
      "metadata": {
        "id": "T0ldly9kXC66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "import wandb\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "s_ZcZc0p_gzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightning"
      ],
      "metadata": {
        "id": "LSjIUzV9f7vA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations"
      ],
      "metadata": {
        "id": "wbnaXuDramPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A"
      ],
      "metadata": {
        "id": "8TAdkxiWataQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating DataLoader"
      ],
      "metadata": {
        "id": "jB25Y9m1_bh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import abstractmethod\n",
        "class ImageRetrievalDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, artifact_id, tokenizer=None, target_size=None, max_length=200, lazy_loading=False):\n",
        "    super().__init__()\n",
        "    self.artifact_id = artifact_id\n",
        "    self.target_size = target_size\n",
        "    self.max_length = max_length\n",
        "    self.lazy_loading = lazy_loading\n",
        "    self.image_files, self.captions = self.fetch_dataset()\n",
        "    self.images = self.image_files\n",
        "\n",
        "    assert tokenizer is not None\n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "    self.tokenized_captions = tokenizer(\n",
        "        list(self.captions), padding=True, truncation=True,\n",
        "        max_length=self.max_length, return_tensors='pt'\n",
        "    )\n",
        "    self.transforms = A.Compose([\n",
        "        A.Resize(target_size, target_size, always_apply=True),\n",
        "        A.Normalize(max_pixel_value=255.0, always_apply=True)\n",
        "    ])\n",
        "\n",
        "  @abstractmethod\n",
        "  def fetch_dataset():\n",
        "    pass\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.captions)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    item = {\n",
        "        key: values[index]\n",
        "        for key, values in self.tokenized_captions.items()\n",
        "    }\n",
        "    image = cv2.imread(self.image_files[index])\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = self.transforms(image=image)[\"image\"]\n",
        "    item[\"image\"] = torch.tensor(image).permute(2, 0, 1).float()\n",
        "    item[\"caption\"] = self.captions[index]\n",
        "    return item"
      ],
      "metadata": {
        "id": "-MLmZPOJ-0Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Filckr8kDataset(ImageRetrievalDataset):\n",
        "  def __init__(self, artifact_id, tokenizer=None, target_size=None, max_length=100, lazy_loading=False):\n",
        "    super().__init__(artifact_id, tokenizer, target_size, max_length, lazy_loading)\n",
        "\n",
        "  def fetch_dataset(self):\n",
        "    if wandb.run is None:\n",
        "      api = wandb.Api()\n",
        "      artifact = api.artifact(self.artifact_id, type=\"dataset\")\n",
        "    else:\n",
        "      articact = wandb.use_artifact(self.artifact_id, type=\"dataset\")\n",
        "\n",
        "    artifact_dir = artifact.download()\n",
        "    annotations = pd.read_csv(os.path.join(artifact_dir, \"captions.txt\"))\n",
        "    image_files = [\n",
        "        os.path.join(artifact_dir, \"Images\", image_file)\n",
        "        for image_file in annotations[\"image\"].to_list()\n",
        "    ]\n",
        "    for image_file in image_files:\n",
        "      assert os.path.isfile(image_file)\n",
        "    captions = annotations[\"caption\"].to_list()\n",
        "    return image_files, captions"
      ],
      "metadata": {
        "id": "iJhlb0CGTzcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataModule"
      ],
      "metadata": {
        "id": "yc7P7V0GXTkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "#from pytorch_lightning import LightningDataModule # pygments error\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "DATASET_LOOKUP = {\n",
        "    \"flickr8k\":  Filckr8kDataset\n",
        "}\n",
        "\n",
        "class ImageRetrievalDataModule(lightning.LightningDataModule):\n",
        "  def __init__(\n",
        "      self,\n",
        "      artifact_id: str,\n",
        "      dataset_name: str, \n",
        "      val_split: float = 0.2,\n",
        "      tokenizer_alias: Optional[str] = None,\n",
        "      target_size: int = 224,\n",
        "      max_length: int = 100,\n",
        "      lazy_loading: bool = False,\n",
        "      train_batch_size: int = 16,\n",
        "      val_batch_size: int = 16,\n",
        "      num_workers: int = 4,\n",
        "      **kwargs,\n",
        "  ):\n",
        "    super().__init__(**kwargs)\n",
        "    self.artifact_id = artifact_id\n",
        "    self.dataset_name = dataset_name\n",
        "    self.val_split = val_split\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_alias)\n",
        "    self.target_size = target_size\n",
        "    self.max_length = max_length\n",
        "    self.lazy_loading = lazy_loading\n",
        "    self.train_batch_size = train_batch_size\n",
        "    self.val_batch_size = val_batch_size\n",
        "    self.num_workers = num_workers\n",
        "\n",
        "  def prepare_data(self):\n",
        "    pass\n",
        "\n",
        "  @staticmethod\n",
        "  def split_data(dataset: ImageRetrievalDataset, val_split: float):\n",
        "    train_length = int((1 - val_split) * len(dataset))\n",
        "    val_length = len(dataset) - train_length\n",
        "    train_dataset, val_dataset = random_split(\n",
        "        dataset, lengths=[train_length, val_length]\n",
        "    )\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "  def setup(\n",
        "      self,\n",
        "      stage: Optional[str] = None,\n",
        "  ) -> None:\n",
        "    dataset = DATASET_LOOKUP[self.dataset_name](\n",
        "        artifact_id=self.artifact_id,\n",
        "        tokenizer=self.tokenizer,\n",
        "        target_size=self.target_size,\n",
        "        max_length=self.max_length,\n",
        "        lazy_loading=self.lazy_loading,\n",
        "    )\n",
        "    self.train_dataset, self.val_dataset = self.split_data(dataset, val_split=self.val_split)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(\n",
        "        self.train_dataset,\n",
        "        batch_size=self.train_batch_size,\n",
        "        num_workers=self.num_workers\n",
        "    )\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(\n",
        "        self.val_dataset,\n",
        "        batch_size=self.val_batch_size,\n",
        "        num_workers=self.num_workers,\n",
        "    )"
      ],
      "metadata": {
        "id": "a3JqxGCgVkSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Encoder"
      ],
      "metadata": {
        "id": "LjlvhhiVKcCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "id": "SceoC58zKlZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "H7Q-QTHCfa_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "  def __init__(\n",
        "      self, model_name: str, pretrained: bool = True, trainable: bool = True,\n",
        "  ) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    self.model = timm.create_model(\n",
        "        model_name, pretrained=pretrained, num_classes=0, global_pool='avg'\n",
        "    )\n",
        "\n",
        "    for p in self.model.parameters():\n",
        "      p.requires_grad = trainable\n",
        "\n",
        "    self.target_token_idx = 0\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.model(x)"
      ],
      "metadata": {
        "id": "K8dR5cQ6Ko64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Encoder"
      ],
      "metadata": {
        "id": "zXVFeL_qMKho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "kobKSosEMCvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "  def __init__(self, model_name: str, trainable: bool = True) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    self.model = transformers.AutoModel.from_pretrained(model_name)\n",
        "\n",
        "    for p in self.model.parameters():\n",
        "      p.requires_grad = trainable\n",
        "\n",
        "    self.target_token_idx = 0\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    last_hidden_state = output.last_hidden_state\n",
        "    return last_hidden_state[:, self.target_token_idx, :]"
      ],
      "metadata": {
        "id": "sTmM1v30MRA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Projection Head"
      ],
      "metadata": {
        "id": "Najssq28NSrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "Sn0pCeMqM_fQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionHead(nn.Module):\n",
        "  def __init__(self, embedding_dim: int, projection_dim: int, dropout: float) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    self.projection = nn.Linear(embedding_dim, projection_dim)\n",
        "    self.gelu = nn.GELU()\n",
        "    self.fc = nn.Linear(projection_dim, projection_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.layer_norm = nn.LayerNorm(projection_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    projected = self.projection(x)\n",
        "    x = self.gelu(projected)\n",
        "    x = self.fc(x)\n",
        "    x = self.dropout(x)\n",
        "    x += projected\n",
        "    return self.layer_norm(x)"
      ],
      "metadata": {
        "id": "OOHhzAkSNWad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLIP Model"
      ],
      "metadata": {
        "id": "yLSSJiEmOK3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools"
      ],
      "metadata": {
        "id": "xef4ZslHSVEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPDualEncoderModel(lightning.LightningModule):\n",
        "  def __init__(\n",
        "      self,\n",
        "      image_encoder_alias: str,\n",
        "      text_encoder_alias: str,\n",
        "      image_encoder_pretrained: bool = True,\n",
        "      image_encoder_trainable: bool = True,\n",
        "      text_encoder_trainable: bool = True,\n",
        "      image_embedding_dims: int = 2048,\n",
        "      text_embedding_dims: int = 768,\n",
        "      projection_dims: int = 256,\n",
        "      dropout: float = 0.0,\n",
        "      temperature: float = 1.0,\n",
        "      weight_decay: float = 0.0,\n",
        "      head_lr: float = 1e-3,\n",
        "      image_encoder_lr: float = 1e-4,\n",
        "      text_encoder_lr: float = 1e-5,\n",
        "      lr_scheduler_patience: float = 1.0,\n",
        "      lr_scheduler_factor: float = 0.8,\n",
        "      *args,\n",
        "      **kwargs,\n",
        "  ) -> None:\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.image_encoder = ImageEncoder(\n",
        "        model_name=image_encoder_alias,\n",
        "        pretrained=image_encoder_pretrained,\n",
        "        trainable=image_encoder_trainable,\n",
        "    )\n",
        "    self.text_encoder = TextEncoder(\n",
        "        model_name=text_encoder_alias,\n",
        "        trainable=text_encoder_trainable\n",
        "    )\n",
        "    self.image_projection = ProjectionHead(\n",
        "        embedding_dim=image_embedding_dims,\n",
        "        projection_dim=projection_dims,\n",
        "        dropout=dropout\n",
        "    )\n",
        "    self.text_projection = ProjectionHead(\n",
        "        embedding_dim=text_embedding_dims,\n",
        "        projection_dim=projection_dims,\n",
        "        dropout=dropout\n",
        "    )\n",
        "    self.log_softmax = nn.LogSoftmax(dim=-1)\n",
        "    self.temperature = temperature\n",
        "    self.weight_decay = weight_decay\n",
        "    self.head_lr = head_lr\n",
        "    self.image_encoder_lr = image_encoder_lr\n",
        "    self.text_encoder_lr = text_encoder_lr\n",
        "    self.lr_scheduler_patience = lr_scheduler_patience\n",
        "    self.lr_scheduler_factor = lr_scheduler_factor\n",
        "\n",
        "  def _compute_losses(self, image_embeddings, text_embeddings):\n",
        "    logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
        "    images_similarity = image_embeddings @ image_embeddings.T\n",
        "    texts_similarity = text_embeddings @ text_embeddings.T\n",
        "    targets = nn.functional.softmax(\n",
        "        (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
        "    )\n",
        "    images_loss = (-targets.T * self.log_softmax(logits.T)).sum(1)\n",
        "    texts_loss = (-targets * self.log_softmax(logits)).sum(1)\n",
        "    return (images_loss + texts_loss) / 2.0\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    image_features = self.image_encoder(inputs[\"image\"])\n",
        "    text_features = self.text_encoder(\n",
        "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n",
        "    )\n",
        "\n",
        "    image_embeddings = self.image_projection(image_features)\n",
        "    text_embeddings = self.text_projection(text_features)\n",
        "\n",
        "    return image_embeddings, text_embeddings\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    parameters = [\n",
        "        {\"params\": self.image_encoder.parameters(), \"lr\": self.image_encoder_lr},\n",
        "        {\"params\": self.text_encoder.parameters(), \"lr\": self.text_encoder_lr},\n",
        "        {\n",
        "            \"params\": itertools.chain(\n",
        "                self.image_projection.parameters(),\n",
        "                self.text_projection.parameters()\n",
        "            ),\n",
        "            \"lr\": self.head_lr,\n",
        "            \"weight_decay\": self.weight_decay\n",
        "        },\n",
        "    ]\n",
        "    optimizer = torch.optim.Adam(parameters, weight_decay=self.weight_decay)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer=optimizer,\n",
        "        mode=\"min\",\n",
        "        patience=self.lr_scheduler_patience,\n",
        "        factor=self.lr_scheduler_factor\n",
        "    )\n",
        "    return {\n",
        "        \"optimizer\": optimizer,\n",
        "        \"lr_scheduler\": lr_scheduler,\n",
        "        \"monitor\": \"val/loss:\"\n",
        "    }\n",
        "\n",
        "  def training_step(self, batch, *args, **kwargs):\n",
        "    image_embeddings, text_embeddings = self.forward(batch)\n",
        "    loss = self._compute_losses(image_embeddings, text_embeddings).mean()\n",
        "    losses = self.all_gather(loss)\n",
        "    self.train_loss = losses.mean()\n",
        "    self.log(\"train/loss:\", self.train_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
        "    return loss\n",
        "\n",
        "  def validation_step(self, batch, *args, **kwargs):\n",
        "    image_embeddings, text_embeddings = self.forward(batch)\n",
        "    loss = self._compute_losses(image_embeddings, text_embeddings).mean()\n",
        "    losses = self.all_gather(loss)\n",
        "    self.val_loss = losses.mean()\n",
        "    self.log(\"val/loss:\", self.val_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "WeDLCOKAOBvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model"
      ],
      "metadata": {
        "id": "GN3fJsPxUE8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_encoder_alias = \"resnet50\"\n",
        "text_encoder_alias = \"distilbert-base-uncased\"\n",
        "\n",
        "model = CLIPDualEncoderModel(image_encoder_alias, text_encoder_alias)\n",
        "data_module = ImageRetrievalDataModule(\n",
        "    artifact_id=\"wandb/clip.lightning-image_retrieval/flickr-8k:latest\",\n",
        "    dataset_name=\"flickr8k\",\n",
        "    tokenizer_alias=text_encoder_alias,\n",
        "    lazy_loading=True\n",
        ")\n",
        "trainer = lightning.Trainer(\n",
        "    max_epochs=20,\n",
        ")\n",
        "trainer.fit(model, data_module)"
      ],
      "metadata": {
        "id": "ug2arMAKT9Rb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}