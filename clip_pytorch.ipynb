{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset download\n",
        "https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/"
      ],
      "metadata": {
        "id": "3rCvdseu-oQw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lLhMKLvEX_7"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip"
      ],
      "metadata": {
        "id": "D_vCegby-pK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Flickr8k_Dataset.zip"
      ],
      "metadata": {
        "id": "NyU0wIKS-vga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Flickr8k_text.zip"
      ],
      "metadata": {
        "id": "Za7kxo3A-xeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "Iw2hMtA__fpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "cwcp-iyAUUlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "kDs7oa1eXrn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightning -U"
      ],
      "metadata": {
        "id": "T0ldly9kXC66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "import wandb\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "s_ZcZc0p_gzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightning"
      ],
      "metadata": {
        "id": "LSjIUzV9f7vA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating DataLoader"
      ],
      "metadata": {
        "id": "jB25Y9m1_bh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import abstractmethod\n",
        "class ImageRetrievalDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, artifact_id, tokenizer=None, target_size=None, max_length=200, lazy_loading=False):\n",
        "    super().__init__()\n",
        "    self.artifact_id = artifact_id\n",
        "    self.target_size = target_size\n",
        "    self.max_length = max_length\n",
        "    self.lazy_loading = lazy_loading\n",
        "    self.image_files, self.captions = self.fetch_dataset()\n",
        "    self.images = self.image_files\n",
        "\n",
        "    assert tokenizer is not None\n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "    self.tokenized_captions = tokenizer(\n",
        "        list(self.captions), padding=True, truncation=True,\n",
        "        max_length=self.max_length, return_tensors='pt'\n",
        "    )\n",
        "    self.transforms = transforms.Compose([\n",
        "        transforms.Resize(target_size, target_size, always_apply=True),\n",
        "        transforms.Normalize(max_pixel_value=255.0, always_apply=True)\n",
        "    ])\n",
        "\n",
        "  @abstractmethod\n",
        "  def fetch_dataset():\n",
        "    pass\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.captions)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    item = {\n",
        "        key: values[index]\n",
        "        for key, values in self.tokenized_captions.items()\n",
        "    }\n",
        "    image = cv2.imread(self.image_files[index])\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    iamge = self.transforms(image=image)[\"image\"]\n",
        "    item[\"image\"] = torch.tensor(image).permute(2, 0, 1).float()\n",
        "    item[\"caption\"] = self.captions[index]\n",
        "    return item"
      ],
      "metadata": {
        "id": "-MLmZPOJ-0Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Filckr8kDataset(ImageRetrievalDataset):\n",
        "  def __init__(self, artifact_id, tokenizer=None, target_size=None, max_length=100, lazy_loading=False):\n",
        "    super.__init__(artifact_id, tokenizer, target_size, max_length, lazy_loading)\n",
        "\n",
        "  def fetch_dataset(self):\n",
        "    if wandb.run is None:\n",
        "      api = wandb.Api()\n",
        "      artifact = api.artifact(self.artifact_id, type=\"dataset\")\n",
        "    else:\n",
        "      articact = wandb.use_artifact(self.artifact_id, type=\"dataset\")\n",
        "\n",
        "    artifact_dir = artifact.download()\n",
        "    annotations = pd.read_csv(os.path.join(artifact_dir, \"captions.txt\"))\n",
        "    image_files = [\n",
        "        os.path.join(artifact_dir, \"Images\", image_file)\n",
        "        for image_file in annotations[\"image\"].to_list()\n",
        "    ]\n",
        "    for image_file in image_files:\n",
        "      assert os.path.isfile(image_file)\n",
        "    captions = annotations[\"caption\"].to_list()\n",
        "    return image_files, captions"
      ],
      "metadata": {
        "id": "iJhlb0CGTzcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataModule"
      ],
      "metadata": {
        "id": "yc7P7V0GXTkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "#from pytorch_lightning import LightningDataModule # pygments error\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "DATASET_LOOKUP = {\n",
        "    \"flickr8k\":  Filckr8kDataset\n",
        "}\n",
        "\n",
        "class ImageRetrievalDataModule(lightning.LightningDataModule):\n",
        "  def __init__(\n",
        "      self,\n",
        "      artifact_id: str,\n",
        "      dataset_name: str, \n",
        "      val_split: float = 0.2,\n",
        "      tokenizer_alias: Optional[str] = None,\n",
        "      target_size: int = 224,\n",
        "      max_length: int = 100,\n",
        "      lazy_loading: bool = False,\n",
        "      train_batch_size: int = 16,\n",
        "      val_batch_size: int = 16,\n",
        "      num_workers: int = 4,\n",
        "      **kwargs,\n",
        "  ):\n",
        "    super().__init__(**kwargs)\n",
        "    self.artifact_id = artifact_id\n",
        "    self.dataset_name = dataset_name\n",
        "    self.val_split = val_split\n",
        "    self.tokenizer_alias = tokenizer_alias\n",
        "    self.target_size = target_size\n",
        "    self.max_length = max_length\n",
        "    self.lazy_loading = lazy_loading\n",
        "    self.train_batch_size = train_batch_size\n",
        "    self.val_batch_size = val_batch_size\n",
        "    self.num_workers = num_workers\n",
        "\n",
        "  def prepare_data(self):\n",
        "    pass\n",
        "\n",
        "  @staticmethod\n",
        "  def split_data(dataset: ImageRetrievalDataset, val_split: float):\n",
        "    train_length = int((1 - val_split) * len(dataset))\n",
        "    val_length = len(dataset) - train_length\n",
        "    train_dataset, val_dataset = random_split(\n",
        "        dataset, lengths=[train_length, val_length]\n",
        "    )\n",
        "\n",
        "  def setup(\n",
        "      self,\n",
        "      stage: Optional[str] = None,\n",
        "  ) -> None:\n",
        "    dataset = DATASET_LOOKUP[self.dataset_name](\n",
        "        artifact_id=self.artifact_id,\n",
        "        tokenizer=self.tokenizer,\n",
        "        target_size=self.target_size,\n",
        "        max_length=self.max_length,\n",
        "        lazy_loading=self.lazy_loading,\n",
        "    )\n",
        "    self.train_dataset, self.val_dataset = self.split_data(dataset, val_split=self.val_split)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(\n",
        "        self.train_dataset,\n",
        "        batch_size=self.train_batch_size,\n",
        "        num_workers=self.num_workers\n",
        "    )\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(\n",
        "        self.val_dataset,\n",
        "        batch_size=self.val_batch_size,\n",
        "        num_workers=self.num_workers,\n",
        "    )"
      ],
      "metadata": {
        "id": "a3JqxGCgVkSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H7Q-QTHCfa_k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}