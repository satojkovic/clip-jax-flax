{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset download\n",
        "https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/"
      ],
      "metadata": {
        "id": "3rCvdseu-oQw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lLhMKLvEX_7"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip"
      ],
      "metadata": {
        "id": "D_vCegby-pK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Flickr8k_Dataset.zip"
      ],
      "metadata": {
        "id": "NyU0wIKS-vga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Flickr8k_text.zip"
      ],
      "metadata": {
        "id": "Za7kxo3A-xeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "Iw2hMtA__fpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "cwcp-iyAUUlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "kDs7oa1eXrn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pygments -U"
      ],
      "metadata": {
        "id": "T79iAMSzYVQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightning -U"
      ],
      "metadata": {
        "id": "T0ldly9kXC66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "import wandb\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "s_ZcZc0p_gzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating DataLoader"
      ],
      "metadata": {
        "id": "jB25Y9m1_bh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import abstractmethod\n",
        "class ImageRetrievalDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, artifact_id, tokenizer=None, target_size=None, max_length=200, lazy_loading=False):\n",
        "    super().__init__()\n",
        "    self.artifact_id = artifact_id\n",
        "    self.target_size = target_size\n",
        "    self.max_length = max_length\n",
        "    self.lazy_loading = lazy_loading\n",
        "    self.image_files, self.captions = self.fetch_dataset()\n",
        "    self.images = self.image_files\n",
        "\n",
        "    assert tokenizer is not None\n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "    self.tokenized_captions = tokenizer(\n",
        "        list(self.captions), padding=True, truncation=True,\n",
        "        max_length=self.max_length, return_tensors='pt'\n",
        "    )\n",
        "    self.transforms = transforms.Compose([\n",
        "        transforms.Resize(target_size, target_size, always_apply=True),\n",
        "        transforms.Normalize(max_pixel_value=255.0, always_apply=True)\n",
        "    ])\n",
        "\n",
        "  @abstractmethod\n",
        "  def fetch_dataset():\n",
        "    pass\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.captions)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    item = {\n",
        "        key: values[index]\n",
        "        for key, values in self.tokenized_captions.items()\n",
        "    }\n",
        "    image = cv2.imread(self.image_files[index])\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    iamge = self.transforms(image=image)[\"image\"]\n",
        "    item[\"image\"] = torch.tensor(image).permute(2, 0, 1).float()\n",
        "    item[\"caption\"] = self.captions[index]\n",
        "    return item"
      ],
      "metadata": {
        "id": "-MLmZPOJ-0Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Filckr8kDatasert(ImageRetrievalDataset):\n",
        "  def __init__(self, artifact_id, tokenizer=None, target_size=None, max_length=100, lazy_loading=False):\n",
        "    super.__init__(artifact_id, tokenizer, target_size, max_length, lazy_loading)\n",
        "\n",
        "  def fetch_dataset(self):\n",
        "    if wandb.run is None:\n",
        "      api = wandb.Api()\n",
        "      artifact = api.artifact(self.artifact_id, type=\"dataset\")\n",
        "    else:\n",
        "      articact = wandb.use_artifact(self.artifact_id, type=\"dataset\")\n",
        "\n",
        "    artifact_dir = artifact.download()\n",
        "    annotations = pd.read_csv(os.path.join(artifact_dir, \"captions.txt\"))\n",
        "    image_files = [\n",
        "        os.path.join(artifact_dir, \"Images\", image_file)\n",
        "        for image_file in annotations[\"image\"].to_list()\n",
        "    ]\n",
        "    for image_file in image_files:\n",
        "      assert os.path.isfile(image_file)\n",
        "    captions = annotations[\"caption\"].to_list()\n",
        "    return image_files, captions"
      ],
      "metadata": {
        "id": "iJhlb0CGTzcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataModule"
      ],
      "metadata": {
        "id": "yc7P7V0GXTkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from pytorch_lightning import LightningDataModule\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "DATASET_LOOKUP = {\n",
        "    \"flickr8k\":  Flickr8kDataset\n",
        "}"
      ],
      "metadata": {
        "id": "a3JqxGCgVkSn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}